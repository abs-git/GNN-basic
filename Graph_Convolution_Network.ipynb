{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Molecular_predict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNG3aEyrJtCADIcuCxOaaqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abs-git/GNN/blob/main/Molecular_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmwtcwQcwA1e",
        "outputId": "2c7bdda0-1b49-4133-d655-e233a6eff9d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 분자를 molecular graph 형태로 변환하고, 분자의 logP값을 알려주는 패키지 설치\n",
        "\n",
        "!curl -LO  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
        "\n",
        "!conda install -y -c rdkit rdkit "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Uv4HQk7wGS-",
        "outputId": "44a5fa5b-61d7-48e0-88c8-b4a4cd14c007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 72.1M  100 72.1M    0     0   174M      0 --:--:-- --:--:-- --:--:--  174M\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \n",
            "Found conflicts! Looking for incompatible packages.\n",
            "This can take several minutes.  Press CTRL-C to abort.\n",
            "\b\bfailed\n",
            "\n",
            "UnsatisfiableError: The following specifications were found\n",
            "to be incompatible with the existing python installation in your environment:\n",
            "\n",
            "Specifications:\n",
            "\n",
            "  - rdkit -> python[version='2.6.*|2.7.*|3.5.*|3.6.*|>=2.7,<2.8.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|>=3.5,<3.6.0a0|3.4.*']\n",
            "\n",
            "Your python: python=3.9\n",
            "\n",
            "If python is on the left-most side of the chain, that's the version you've asked for.\n",
            "When python appears to the right, that indicates that the thing on the left is somehow\n",
            "not available for the python version you are constrained to. Note that conda will not\n",
            "change your python version to a different minor version unless you explicitly specify\n",
            "that.\n",
            "\n",
            "The following specifications were found to be incompatible with your system:\n",
            "\n",
            "  - feature:/linux-64::__glibc==2.27=0\n",
            "  - feature:|@/linux-64::__glibc==2.27=0\n",
            "  - rdkit -> libgcc-ng[version='>=7.3.0'] -> __glibc[version='>=2.17']\n",
            "\n",
            "Your installed version is: 2.27\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 분자를 텍스트 형태로 표현한 smiles 파일과 molecular graph를 생성하는데 필요한 라이브러리\n",
        "\n",
        "!curl -o ZINC.smiles https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning/master/Lec9/ZINC.smiles\n",
        "!curl -o vocab.npy https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning/master/Lec9/vocab.npy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVe2zevew18X",
        "outputId": "91974071-2d16-4273-abcf-65b8c787bd3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 5374k  100 5374k    0     0  7454k      0 --:--:-- --:--:-- --:--:-- 7454k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   256  100   256    0     0   1333      0 --:--:-- --:--:-- --:--:--  1333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# from rdkit import Chem, DataStructues\n",
        "# from rdkit.Chem import AllChem\n",
        "# from rdkit.Chem.Crippen import MolLogP \n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3PUr4P3HxFxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paser = argparse.ArgumentParser()\n",
        "args = paser.parse_args(\"\")\n",
        "args.seed = 123\n",
        "args.val_size = 0.1\n",
        "args.test_size = 0.1\n",
        "args.shuffle = True"
      ],
      "metadata": {
        "id": "jVjX6Qh-xgIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch setting\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "else:\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')"
      ],
      "metadata": {
        "id": "P1ovo5ljz5El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom torch dataset\n",
        "# 두 개의 input (list_feature, list_adj)로 부터 하나의 output (logP Value) 을 얻고자 한다.\n",
        "\n",
        "class GCNDataset(Dataset):\n",
        "  def __init__(self, list_feature, list_adj, list_logP):\n",
        "    self.list_feature = list_feature\n",
        "    self.list_adj = list_adj\n",
        "    self.list_logP = list_logP\n",
        "\n",
        "  def __len__(self):                # 전체 데이터의 개수\n",
        "    return len(self.list_feature)\n",
        "\n",
        "  def __getitem__(self, index):             # 특정 위치의 값 \n",
        "    return self.list_feature[index], self.list_adj[index], self.list_logP[index]\n",
        "\n",
        "\n",
        "def partition(list_feature, list_adj, list_logP):\n",
        "  # train, validation, test로 나누는 함수\n",
        "\n",
        "  num_total = list_feature.shape[0]\n",
        "  num_train = int(num_total * (1 - args.test_size - args.val_size))   # 0.8\n",
        "  num_val = int(num_total * args.val_size)\n",
        "  num_test = int(num_total * args.test_size)\n",
        "\n",
        "  feature_train = list_feature[ : num_train]\n",
        "  feature_val = list_feature[num_train : num_train + num_val]\n",
        "  feature_test = list_feature[num_total - num_test : ]\n",
        "\n",
        "  adj_train = list_adj[ : num_train]\n",
        "  adj_val = list_adj[num_train : num_train + num_val]\n",
        "  adj_test = list_adj[num_total - num_test : ]\n",
        "\n",
        "  logP_train = list_logP[ : num_train]\n",
        "  logP_val = list_logP[num_train : num_train + num_val]\n",
        "  logP_test = list_logP[num_total - num_test : ]\n",
        "  \n",
        "  train_set = GCNDataset(feature_train, adj_train, logP_train)\n",
        "  val_set = GCNDataset(feature_val, adj_val, logP_val)\n",
        "  test_set = GCNDataset(feature_test, adj_test, logP_test)\n",
        "\n",
        "  partition  = {\n",
        "      'train' : train_set,\n",
        "      'val' : val_set,\n",
        "      'test' : test_set\n",
        "  }"
      ],
      "metadata": {
        "id": "c9A3lnn-0UPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "partition = partition(list_feature, list_adj, list_logP, args)"
      ],
      "metadata": {
        "id": "tE0VLSFK5lGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GCN \n",
        "\n",
        "adjacent matrix (인접 행렬) * (Hidden state * Weights) → Convolution layer와 같은 기능을 함\n",
        "\n",
        "adjacent matrix * Node feature matrix ($H^l$)"
      ],
      "metadata": {
        "id": "Z3a7CuDgv7Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# + Attention module\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "  def __init__(self, in_dim, out_dim, num_head):   # num_head = multi head attention에서 H를 num_head 만큼 서로 다른 weights와 곱한다.\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.num_head = num_head\n",
        "    self.atn_dim = out_dim // num_head\n",
        "\n",
        "    self.linears = nn.ModuleList()\n",
        "    self.corelations - nn.ParameterList()\n",
        "\n",
        "    for i in range(self.num_head):\n",
        "      self.linears.append(nn.Linear(in_dim, self.atn_dim))\n",
        "      corelation = torch.FloatTensor(self.atn_dim, self.atn_dim)\n",
        "      self.corlations.append(nn.Parameter(corelation))\n",
        "\n",
        "    self.tanh = torch.nn.Tanh()\n",
        "\n",
        "  def forward(self, x, adj):\n",
        "    \n",
        "    heads = []\n",
        "    for i in range(self.num_head):\n",
        "      x_transformed = self.linears[i](x)\n",
        "      alpha = self.attention_matrix(x_transformed, self.corelation[i], adj)\n",
        "      x_head = torch.matmul(alpha, x_transformed)\n",
        "      heads.append(x_head)\n",
        "\n",
        "    output = torch.cat(heads, dim = 2)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "  def attention_matrix(self, x_transformed, corelation, adj):\n",
        "    x = torch.einsum('akj, ij -> aki', (x_transformed, corelation))\n",
        "    alpha = torch.matmul(x, torch.transpose(x_transformed, 1, 2))\n",
        "    alpha = torch.mul(alpha, adj)\n",
        "    alpha = self.tanh()\n",
        "\n",
        "    return alpha\n"
      ],
      "metadata": {
        "id": "ihT7lHL1WBID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modules\n",
        "\n",
        "class GCNLayer(nn.Module):          # node feature matrix와 adjacency matrix를 받아 graph convolution 연산을 수행\n",
        "  \n",
        "  def __init__(self, in_dim, out_dim, n_atom, act = None, bn = False):\n",
        "    super(GCNLayer, self).__init__()\n",
        "\n",
        "    self.use_bn = bn\n",
        "    self.linear = nn.Linear(in_dim, out_dim)          # convolution의 역할을 함\n",
        "    nn.init.xavier_uniform_(self.linear.weight)\n",
        "    self.bn = nn.BatchNorm1d(n_atom)\n",
        "    self.activation = act\n",
        "\n",
        "  def forward(self, x, adj):        # 두 개의 input, return 값도 두 개\n",
        "    out = self.linear(x)            # H * W + bias\n",
        "    out = torch.matmul(adj, out)    # adj * (H * W + bias)  내적\n",
        "    \n",
        "    if self.use_bn:\n",
        "      out = self.bn(out)\n",
        "    if self.activation != None:\n",
        "      out = self.activation(out)\n",
        "    \n",
        "    return out, adj\n",
        "\n",
        "\n",
        "class GCNLayer_with_attention(nn.Module):     \n",
        "  \n",
        "  def __init__(self, in_dim, out_dim, n_atom, act = None, bn = False, atn = False, num_head = 1):\n",
        "    super(GCNLayer_with_attention, self).__init__()\n",
        "\n",
        "    self.use_bn = bn\n",
        "    self.linear = nn.Linear(in_dim, out_dim)          # convolution의 역할을 함\n",
        "    nn.init.xavier_uniform_(self.linear.weight)\n",
        "    self.bn = nn.BatchNorm1d(n_atom)\n",
        "    self.activation = act\n",
        "\n",
        "    self.use_atn = atn\n",
        "    self.attention = Attention(out_dim, out_dim, num_head)\n",
        "\n",
        "\n",
        "  def forward(self, x, adj):        # 두 개의 input, return 값도 두 개\n",
        "    out = self.linear(x)            # H * W + bias\n",
        "\n",
        "    if self.use_atn:\n",
        "      out = self.attention(out, adj)\n",
        "    else :\n",
        "      out = torch.matmul(adj, out)    # adj * (H * W + bias)  내적\n",
        "    \n",
        "    if self.use_bn:\n",
        "      out = self.bn(out)\n",
        "    if self.activation != None:\n",
        "      out = self.activation(out)\n",
        "    \n",
        "    return out, adj\n",
        "\n"
      ],
      "metadata": {
        "id": "E9wB3fkH5vFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "class SkipConnection(nn.Module):              # skip connection module\n",
        "\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    super(SkipConnection, self).__init__()\n",
        "\n",
        "    self.in_dim = in_dim\n",
        "    self.out_dim = out_dim\n",
        "\n",
        "    self.linear = nn.Linear(in_dim, out_dim, bias = False)\n",
        "\n",
        "  def forward(self, in_x, out_x):\n",
        "    if (self.in_dim != self.out_dim):\n",
        "      in_x = self.linear(in_x)\n",
        "    \n",
        "    out = in_x + out_x\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class GatedSkipConnection(nn.Module):\n",
        "\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    super(GatedSkipConnection, self).__init__()\n",
        "\n",
        "    self.in_dim = in_dim\n",
        "    self.out_dim = out_dim\n",
        "\n",
        "    self.linear = nn.Linear(in_dim, out_dim, bias = False)\n",
        "    \n",
        "    self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
        "    self.linear_coef_out = nn.Linear(out_dim, out_dim)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, in_x, out_x):\n",
        "    if (self.in_dim != self.out_dim):\n",
        "      in_x = self.linear(in_x)\n",
        "\n",
        "    z = self.gate_coefficient(in_x, out_x)\n",
        "    out = torch.mul(z, out_x) + torch.mul(1.0 -z, in_x)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def gate_coefficient(self, in_x, out_x):\n",
        "    x1 = self.linear_coef_in(in_x)\n",
        "    x2 = self.linear_coef_out(out_x)\n",
        "\n",
        "    return self.sigmoid(x1 + x2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tIiMflPJ8tuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GCNBlock(nn.Module):            # GCN layers와 skip or gated skip connection 를 조합한 블럭\n",
        "  \n",
        "  def __init__(self, n_layer, in_dim, hidden_dim, out_dim, n_atom, bn = True, sc = 'gsc', act = None):\n",
        "    super(GCNBlock, self).__init__()\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "    for i in range(n_layer):\n",
        "      self.layers.append(GCNLayer(in_dim if i == 0 else hidden_dim,                 # 첫째, 마지막 layer 빼곤 hidden layer\n",
        "                                  out_dim if i == n_layer - 1 else hidden_dim,\n",
        "                                  n_atom,\n",
        "                                  act if i != n_layer - 1 else None,\n",
        "                                  bn\n",
        "                                  ))\n",
        "\n",
        "      self.relu = nn.ReLU()\n",
        "      if sc == 'gsc':\n",
        "        self.sc = GatedSkipConnection(in_dim, out_dim)\n",
        "      elif sc == 'sc' :\n",
        "        self.sc = SkipConnection(in_dim, out_dim)\n",
        "      elif sc == 'no':\n",
        "        self.sc = None\n",
        "      else:\n",
        "        assert False, \"Check the skip connection type.\"\n",
        "\n",
        "    self.activation = act\n",
        "  \n",
        "  def forward(self, x, adj):\n",
        "    identity = x\n",
        "\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      out, adj = layer((x if i == 0 else out), adj)\n",
        "    \n",
        "    if self.sc != None:\n",
        "      out = self.sc(identity, out)\n",
        "\n",
        "    if self.activation != None:\n",
        "      out = self.activation(out)\n",
        "\n",
        "    return out, adj\n",
        "\n"
      ],
      "metadata": {
        "id": "gNPNP7gICqt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 같은 그래프지만 노드에 부여된 번호에 따라 행렬이 변화가 되는 문제를 해결하기 위해 Readout layer를 거친다.\n",
        "\n",
        "\n",
        "class ReadOut(nn.Module):             # graph structrure에 permutation invariance를 주기 위하여 linear layer를 거친 뒤 batch 별로 summation하는 module\n",
        "                                      # MLP 통과 후 합치는 것과 같음\n",
        "  def __init__(self, in_dim, out_dim, act = None):\n",
        "    super(ReadOut, self).__init__()\n",
        "\n",
        "    self.in_dim = in_dim\n",
        "    self.out_dim = out_dim\n",
        "\n",
        "    self.linear = nn.Linear(self.in_dim,\n",
        "                            self.out_dim)\n",
        "    nn.init.xavier_uniform_(self.linear.weight)\n",
        "    self.activation = act\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.linear(x)\n",
        "      out = torch.sum(out, 1)\n",
        "\n",
        "      if self.activation != None:\n",
        "        out = self.activation(out)\n",
        "      \n",
        "      return out\n",
        "\n",
        "\n",
        "class Predictor(nn.Module):\n",
        "\n",
        "  def __init__(self, in_dim, out_dim, act = None):\n",
        "    super(Predictor, self).__init__()\n",
        "\n",
        "    self.in_dim = in_dim\n",
        "    self.out_dim = out_dim\n",
        "\n",
        "    self.linear = nn.Linear(self.in_dim,\n",
        "                            self.out_dim)\n",
        "    nn.init.xavier_uniform_(self.linear.weight)\n",
        "    self.activation = act\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.linear(x)\n",
        "\n",
        "    if self.activation != None:\n",
        "      out = self.activation(out)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "lJN9kwCRFy-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "\n",
        "class GCNNet(nn.Module):\n",
        "\n",
        "  def __init__(self, args):\n",
        "    super(GCNNet, self).__init__()\n",
        "\n",
        "    self.blocks = nn.ModuleList()\n",
        "    for i in range(args.n_block):\n",
        "      self.blocks.append(GCNBlock(args.n_layer,\n",
        "                                  args.in_dim if i == 0 else args.hidden_dim,\n",
        "                                  args.hidden_dim,\n",
        "                                  args.hidden_dim,\n",
        "                                  args.n_atom,\n",
        "                                  args.bn,\n",
        "                                  args.sc,\n",
        "                                  args.act                                  \n",
        "                                  ))\n",
        "      self.readout = ReadOut(args.hidden_dim,\n",
        "                             args.pred_dim1,\n",
        "                             act = nn.ReLU()\n",
        "                             )\n",
        "      self.pred1 = Predictor(args.pred_dim1,\n",
        "                             args.pred_dim2,\n",
        "                             act = nn.ReLU()\n",
        "                             )\n",
        "      self.pred2 = Predictor(args.pred_dim2,\n",
        "                             args.pred_dim3,\n",
        "                             act = nn.Tanh()\n",
        "                             )\n",
        "      self.pred3 = Predictor(args.pred_dim3,\n",
        "                             args.out_dim\n",
        "                             )\n",
        "\n",
        "  def forward(self, x, adj):\n",
        "    for i, block in enumerate(self.blocks):\n",
        "      out, adj = block((x if i == 0 else out), adj)\n",
        "    \n",
        "    out = self.readout(out)\n",
        "    out = self.pred1(out)\n",
        "    out = self.pred2(out)\n",
        "    out = self.pred3(out)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "k91i0k5CJmMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, validation, test and experiment\n",
        "\n",
        "def train(net, partition, optimizer, criterion, args):\n",
        "  train_loader = torch.utils.data.DataLoader(partition['train'],\n",
        "                                             batch_size = args.train_batch_size,\n",
        "                                             shuffle = True,\n",
        "                                             num_workers = 2\n",
        "                                             )\n",
        "  net.train()\n",
        "  optimizer.zero.grad()\n",
        "\n",
        "  train_loss = 0.0\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    list_feature, list_adj, list_logP = data\n",
        "\n",
        "    list_feature = list_feature.cuda().float()\n",
        "    list_adj = list_adj.cuda().float()\n",
        "    list_logP = list_logP.cuda().float().view(-1, 1)\n",
        "\n",
        "    outputs = net(list_feature, list_adj)\n",
        "\n",
        "    loss = criterion(outputs, list_logP)        # mse loss\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss = train_loss / len(train_loader)\n",
        "\n",
        "  return net, train_loss\n"
      ],
      "metadata": {
        "id": "T-p9wnSSL4Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def validation(net, partition, optimizer, criterion, args):\n",
        "  val_loader = torch.utils.data.DataLoader(partition['val'],\n",
        "                                             batch_size = args.test_batch_size,\n",
        "                                             shuffle = True,\n",
        "                                             num_workers = 2\n",
        "                                             )\n",
        "  net.eval()\n",
        "\n",
        "  val_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for data in val_loader:\n",
        "      list_feature, list_adj, list_logP = data\n",
        "\n",
        "      list_feature = list_feature.cuda().float()\n",
        "      list_adj = list_adj.cuda().float()\n",
        "      list_logP = list_logP.cuda().float().view(-1, 1)\n",
        "\n",
        "      outputs = net(list_feature, list_adj)\n",
        "\n",
        "      loss = criterion(outputs, list_logP)      # mse loss\n",
        "\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "\n",
        "  return val_loss\n"
      ],
      "metadata": {
        "id": "wZ5kE5G2Nz2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 값을 예측하는 모델이기 때문에 accuracy가 아닌 mae, mean std를 평가 지표로 활용한다.\n",
        "\n",
        "def test(net, partition, args):\n",
        "  test_loader = torch.utils.data.DataLoader(partition['test'],\n",
        "                                            batch_size = args.test_batch_size,\n",
        "                                            shuffle = False, num_workers = 2\n",
        "                                            )\n",
        "\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    logP_total = []\n",
        "    pred_logP_total = []\n",
        "    for data in test_loader:\n",
        "      list_feature, list_adj, list_logP = data\n",
        "\n",
        "      list_feature = list_feature.cuda().float()\n",
        "      list_adj = list_adj.cuda().float()\n",
        "      list_logP = list_logP.cuda().float()\n",
        "\n",
        "      logP_total += list_logP.tolist()\n",
        "      list_logP = list_logP.view(-1, 1)\n",
        "\n",
        "      outputs = net(list_feature, list_adj)\n",
        "      pred_logP_total += outputs.view(-1).tolist()\n",
        "\n",
        "    mae = mean_absolute_error(logP_total, pred_logP_total)\n",
        "    std = np.std(np.array(logP_total) - np.array(pred_logP_total))\n",
        "\n",
        "  return mae, std, logP_total, pred_logP_total\n"
      ],
      "metadata": {
        "id": "xilAf5SGO_Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def experiment(partition, args):\n",
        "  \n",
        "  net = GCNNet(args)\n",
        "  net.cuda()\n",
        "\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = optim.Adam(net.parameters(), lr = args.lr)\n",
        "\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  for epoch in range(args.EPOCH):\n",
        "    t_start = time.time()\n",
        "\n",
        "    net, train_loss = train(net, partition, optimizer, criterion, args)\n",
        "    val_loss = validation(net, partition, criterion, args)\n",
        "\n",
        "    t_end = time.time()\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print('epoch : {}, loss (train/val) : {:.4f}/{:.4f}, time : {.2f} sec'.format(epoch, train_loss, val_loss, t_end - t_start))\n",
        "\n",
        "  mae, std, _, _ = test(net, partition, args)\n",
        "\n",
        "  result = {}\n",
        "  result['train_losses'] = train_losses\n",
        "  result['val_losses'] = val_losses\n",
        "  result['mae'] = mae\n",
        "  result['std'] = std\n",
        "  \n",
        "  return vars(args), result\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sigoa4s_Q4e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_B9ta2T3UN3L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
